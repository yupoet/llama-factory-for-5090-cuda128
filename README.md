# LLaMA-Factory for CUDA 12.8+ (RTX 5090 / RTX PRO6000)

# LLaMA-Factory é€‚ç”¨äº CUDA 12.8+ (RTX 5090 / RTX PRO6000)

[English](#english) | [ä¸­æ–‡](#ä¸­æ–‡)

---

<a name="english"></a>
## ğŸ‡ºğŸ‡¸ English

### Introduction

A ready-to-use AutoDL image for LLaMA-Factory, specifically optimized for **NVIDIA RTX 5090** and **RTX PRO6000** GPUs with **CUDA 12.8** support. This image comes pre-configured with all necessary dependencies, including Flash Attention 2, DeepSpeed, and quantization tools.

### âœ¨ Features

- ğŸš€ **Blackwell Architecture Support** - Full support for RTX 5090/PRO6000 (SM 100)
- âš¡ **Flash Attention 2** - Pre-installed for maximum training efficiency
- ğŸ”§ **DeepSpeed Integration** - Ready for distributed training
- ğŸ“¦ **Quantization Tools** - BitsAndBytes for 4-bit/8-bit training
- ğŸŒ **China Mirror Pre-configured** - Fast downloads within mainland China

### ğŸ“¦ Environment Specifications

#### System Environment

| Component | Version | Description |
|-----------|---------|-------------|
| **OS** | Ubuntu 22.04 | Base operating system |
| **Python** | 3.12 | Python runtime |
| **CUDA** | 12.8 | NVIDIA CUDA Toolkit |
| **cuDNN** | 9.x | Deep Neural Network library |

#### Core Dependencies

| Component | Version | Description |
|-----------|---------|-------------|
| **PyTorch** | 2.8.0+cu128 | Deep learning framework |
| **LLaMA-Factory** | 0.9.3 | Fine-tuning framework |
| **Transformers** | 4.52.4 | Hugging Face Transformers |
| **PEFT** | 0.15.2 | Parameter-Efficient Fine-Tuning |
| **TRL** | 0.9.6 | Transformer Reinforcement Learning |
| **Accelerate** | 1.7.0 | Hugging Face Accelerate |

#### Acceleration & Optimization

| Component | Version | Description |
|-----------|---------|-------------|
| **Flash Attention** | 2.8.3 | Memory-efficient attention |
| **DeepSpeed** | 0.16.9 | Distributed training optimization |
| **BitsAndBytes** | 0.49.0 | 4-bit/8-bit quantization |
| **Triton** | 3.4.0 | GPU compiler |

#### Data Processing

| Component | Version | Description |
|-----------|---------|-------------|
| **Datasets** | 3.6.0 | Hugging Face Datasets |
| **Tokenizers** | 0.21.1 | Fast tokenizers |
| **SentencePiece** | 0.2.1 | Subword tokenization |
| **TikToken** | 0.12.0 | OpenAI's tokenizer |

### ğŸ“ Directory Structure

```
/
â”œâ”€â”€ root/
â”‚   â”œâ”€â”€ miniconda3/                    # Conda environment (system disk)
â”‚   â”‚   â””â”€â”€ lib/python3.12/site-packages/
â”‚   â”‚       â””â”€â”€ llamafactory/          # LLaMA-Factory installation
â”‚   â”œâ”€â”€ autodl-tmp/                    # Data disk (fast I/O)
â”‚   â”‚   â””â”€â”€ models/                    # Recommended model storage
â”‚   â””â”€â”€ autodl-fs/                     # Shared file storage
â”‚       â””â”€â”€ models/                    # Alternative model storage
```

### ğŸš€ Quick Start

#### 1. Launch WebUI

```bash
GRADIO_SERVER_NAME=0.0.0.0 GRADIO_SERVER_PORT=6006 llamafactory-cli webui
```

Then access the WebUI via **port 6006** in AutoDL's "Custom Service" panel.

#### 2. Download Models (with China Mirror)

```bash
# Set HuggingFace mirror
export HF_ENDPOINT=https://hf-mirror.com

# Download Qwen2.5-7B-Instruct
huggingface-cli download Qwen/Qwen2.5-7B-Instruct \
    --local-dir /root/autodl-tmp/models/Qwen2.5-7B-Instruct

# Download Llama-3-8B-Instruct
huggingface-cli download meta-llama/Meta-Llama-3-8B-Instruct \
    --local-dir /root/autodl-tmp/models/Llama-3-8B-Instruct
```

#### 3. Alternative: ModelScope Download

```bash
pip install modelscope

python -c "
from modelscope import snapshot_download
model_dir = snapshot_download('qwen/Qwen2.5-7B-Instruct', cache_dir='/root/autodl-tmp/models')
print(f'Model downloaded to: {model_dir}')
"
```

### âš™ï¸ Pre-configured Mirrors

| Service | Mirror URL |
|---------|------------|
| **pip** | https://mirrors.aliyun.com/pypi/simple/ |
| **pip (backup)** | https://pypi.tuna.tsinghua.edu.cn/simple/ |
| **HuggingFace** | https://hf-mirror.com |

### ğŸ”§ CLI Commands

```bash
# Check version
llamafactory-cli version

# Launch WebUI
llamafactory-cli webui

# Start training (CLI mode)
llamafactory-cli train examples/train_lora/qwen2_lora_sft.yaml

# Chat with model
llamafactory-cli chat --model_name_or_path /root/autodl-tmp/models/Qwen2.5-7B-Instruct

# Export model
llamafactory-cli export --model_name_or_path /path/to/model --export_dir /path/to/export
```

### ğŸ“Š Supported GPUs

| GPU | VRAM | Architecture | Compute Capability |
|-----|------|--------------|-------------------|
| RTX 5090 | 32GB | Blackwell | SM 100 |
| RTX PRO6000 | 48GB | Blackwell | SM 100 |
| RTX 4090 | 24GB | Ada Lovelace | SM 89 |
| RTX 4080 | 16GB | Ada Lovelace | SM 89 |

### âš ï¸ Important Notes

1. **Data Storage**: Store large models on `/root/autodl-tmp/` (data disk) for better I/O performance
2. **Persistent Storage**: Files in `/root/autodl-fs/` persist across instances
3. **Port Access**: Use port 6006 or 6008 for WebUI access via AutoDL's custom service
4. **Background Running**: Use `screen` or `nohup` for long-running training jobs

```bash
# Using screen
screen -S llama
GRADIO_SERVER_NAME=0.0.0.0 GRADIO_SERVER_PORT=6006 llamafactory-cli webui
# Press Ctrl+A, then D to detach

# Reconnect
screen -r llama
```

---

<a name="ä¸­æ–‡"></a>
## ğŸ‡¨ğŸ‡³ ä¸­æ–‡

### ç®€ä»‹

è¿™æ˜¯ä¸€ä¸ªä¸“ä¸º **NVIDIA RTX 5090** å’Œ **RTX PRO6000** æ˜¾å¡ä¼˜åŒ–çš„ AutoDL é•œåƒï¼Œæ”¯æŒ **CUDA 12.8**ï¼Œé¢„è£…äº† LLaMA-Factory åŠæ‰€æœ‰å¿…è¦ä¾èµ–ï¼ŒåŒ…æ‹¬ Flash Attention 2ã€DeepSpeed å’Œé‡åŒ–å·¥å…·ï¼Œå¼€ç®±å³ç”¨ã€‚

### âœ¨ ç‰¹æ€§

- ğŸš€ **Blackwell æ¶æ„æ”¯æŒ** - å®Œæ•´æ”¯æŒ RTX 5090/PRO6000 (SM 100)
- âš¡ **Flash Attention 2** - é¢„è£…ï¼Œæœ€å¤§åŒ–è®­ç»ƒæ•ˆç‡
- ğŸ”§ **DeepSpeed é›†æˆ** - æ”¯æŒåˆ†å¸ƒå¼è®­ç»ƒ
- ğŸ“¦ **é‡åŒ–å·¥å…·** - BitsAndBytes æ”¯æŒ 4-bit/8-bit è®­ç»ƒ
- ğŸŒ **å›½å†…é•œåƒå·²é…ç½®** - ä¸­å›½å¤§é™†é«˜é€Ÿä¸‹è½½

### ğŸ“¦ ç¯å¢ƒé…ç½®

#### ç³»ç»Ÿç¯å¢ƒ

| ç»„ä»¶ | ç‰ˆæœ¬ | è¯´æ˜ |
|------|------|------|
| **æ“ä½œç³»ç»Ÿ** | Ubuntu 22.04 | åŸºç¡€æ“ä½œç³»ç»Ÿ |
| **Python** | 3.12 | Python è¿è¡Œæ—¶ |
| **CUDA** | 12.8 | NVIDIA CUDA å·¥å…·åŒ… |
| **cuDNN** | 9.x | æ·±åº¦ç¥ç»ç½‘ç»œåº“ |

#### æ ¸å¿ƒä¾èµ–

| ç»„ä»¶ | ç‰ˆæœ¬ | è¯´æ˜ |
|------|------|------|
| **PyTorch** | 2.8.0+cu128 | æ·±åº¦å­¦ä¹ æ¡†æ¶ |
| **LLaMA-Factory** | 0.9.3 | å¾®è°ƒæ¡†æ¶ |
| **Transformers** | 4.52.4 | Hugging Face Transformers |
| **PEFT** | 0.15.2 | å‚æ•°é«˜æ•ˆå¾®è°ƒ |
| **TRL** | 0.9.6 | å¼ºåŒ–å­¦ä¹ è®­ç»ƒ |
| **Accelerate** | 1.7.0 | Hugging Face åŠ é€Ÿåº“ |

#### åŠ é€Ÿä¸ä¼˜åŒ–ç»„ä»¶

| ç»„ä»¶ | ç‰ˆæœ¬ | è¯´æ˜ |
|------|------|------|
| **Flash Attention** | 2.8.3 | å†…å­˜é«˜æ•ˆæ³¨æ„åŠ›æœºåˆ¶ |
| **DeepSpeed** | 0.16.9 | åˆ†å¸ƒå¼è®­ç»ƒä¼˜åŒ– |
| **BitsAndBytes** | 0.49.0 | 4-bit/8-bit é‡åŒ– |
| **Triton** | 3.4.0 | GPU ç¼–è¯‘å™¨ |

#### æ•°æ®å¤„ç†ç»„ä»¶

| ç»„ä»¶ | ç‰ˆæœ¬ | è¯´æ˜ |
|------|------|------|
| **Datasets** | 3.6.0 | Hugging Face æ•°æ®é›† |
| **Tokenizers** | 0.21.1 | å¿«é€Ÿåˆ†è¯å™¨ |
| **SentencePiece** | 0.2.1 | å­è¯åˆ†è¯ |
| **TikToken** | 0.12.0 | OpenAI åˆ†è¯å™¨ |

### ğŸ“ ç›®å½•ç»“æ„

```
/
â”œâ”€â”€ root/
â”‚   â”œâ”€â”€ miniconda3/                    # Conda ç¯å¢ƒï¼ˆç³»ç»Ÿç›˜ï¼‰
â”‚   â”‚   â””â”€â”€ lib/python3.12/site-packages/
â”‚   â”‚       â””â”€â”€ llamafactory/          # LLaMA-Factory å®‰è£…ä½ç½®
â”‚   â”œâ”€â”€ autodl-tmp/                    # æ•°æ®ç›˜ï¼ˆé«˜é€Ÿ I/Oï¼‰
â”‚   â”‚   â””â”€â”€ models/                    # æ¨èçš„æ¨¡å‹å­˜æ”¾ä½ç½®
â”‚   â””â”€â”€ autodl-fs/                     # å…±äº«æ–‡ä»¶å­˜å‚¨
â”‚       â””â”€â”€ models/                    # å¤‡é€‰æ¨¡å‹å­˜æ”¾ä½ç½®
```

### ğŸš€ å¿«é€Ÿå¼€å§‹

#### 1. å¯åŠ¨ WebUI

```bash
GRADIO_SERVER_NAME=0.0.0.0 GRADIO_SERVER_PORT=6006 llamafactory-cli webui
```

å¯åŠ¨åï¼Œåœ¨ AutoDL æ§åˆ¶å°ç‚¹å‡»ã€Œè‡ªå®šä¹‰æœåŠ¡ã€çš„ **6006 ç«¯å£** è®¿é—® WebUIã€‚

#### 2. ä¸‹è½½æ¨¡å‹ï¼ˆä½¿ç”¨å›½å†…é•œåƒï¼‰

```bash
# è®¾ç½® HuggingFace é•œåƒ
export HF_ENDPOINT=https://hf-mirror.com

# ä¸‹è½½ Qwen2.5-7B-Instruct
huggingface-cli download Qwen/Qwen2.5-7B-Instruct \
    --local-dir /root/autodl-tmp/models/Qwen2.5-7B-Instruct

# ä¸‹è½½ Llama-3-8B-Instruct
huggingface-cli download meta-llama/Meta-Llama-3-8B-Instruct \
    --local-dir /root/autodl-tmp/models/Llama-3-8B-Instruct
```

#### 3. å¤‡é€‰ï¼šä½¿ç”¨ ModelScope ä¸‹è½½

```bash
pip install modelscope

python -c "
from modelscope import snapshot_download
model_dir = snapshot_download('qwen/Qwen2.5-7B-Instruct', cache_dir='/root/autodl-tmp/models')
print(f'æ¨¡å‹å·²ä¸‹è½½åˆ°: {model_dir}')
"
```

### âš™ï¸ å·²é…ç½®çš„å›½å†…é•œåƒ

| æœåŠ¡ | é•œåƒåœ°å€ |
|------|----------|
| **pip ä¸»æº** | https://mirrors.aliyun.com/pypi/simple/ |
| **pip å¤‡ç”¨** | https://pypi.tuna.tsinghua.edu.cn/simple/ |
| **HuggingFace** | https://hf-mirror.com |

### ğŸ”§ å¸¸ç”¨å‘½ä»¤

```bash
# æŸ¥çœ‹ç‰ˆæœ¬
llamafactory-cli version

# å¯åŠ¨ WebUI
llamafactory-cli webui

# å‘½ä»¤è¡Œè®­ç»ƒ
llamafactory-cli train examples/train_lora/qwen2_lora_sft.yaml

# æ¨¡å‹å¯¹è¯
llamafactory-cli chat --model_name_or_path /root/autodl-tmp/models/Qwen2.5-7B-Instruct

# å¯¼å‡ºæ¨¡å‹
llamafactory-cli export --model_name_or_path /path/to/model --export_dir /path/to/export
```

### ğŸ“Š æ”¯æŒçš„æ˜¾å¡

| æ˜¾å¡ | æ˜¾å­˜ | æ¶æ„ | ç®—åŠ› |
|------|------|------|------|
| RTX 5090 | 32GB | Blackwell | SM 100 |
| RTX PRO6000 | 48GB | Blackwell | SM 100 |
| RTX 4090 | 24GB | Ada Lovelace | SM 89 |
| RTX 4080 | 16GB | Ada Lovelace | SM 89 |

### âš ï¸ é‡è¦æç¤º

1. **æ•°æ®å­˜å‚¨**ï¼šå»ºè®®å°†å¤§æ¨¡å‹å­˜æ”¾åœ¨ `/root/autodl-tmp/`ï¼ˆæ•°æ®ç›˜ï¼‰ï¼ŒI/O æ€§èƒ½æ›´å¥½
2. **æŒä¹…å­˜å‚¨**ï¼š`/root/autodl-fs/` ä¸­çš„æ–‡ä»¶åœ¨å®ä¾‹é—´å…±äº«ä¸”æŒä¹…ä¿å­˜
3. **ç«¯å£è®¿é—®**ï¼šé€šè¿‡ AutoDLã€Œè‡ªå®šä¹‰æœåŠ¡ã€è®¿é—® 6006 æˆ– 6008 ç«¯å£
4. **åå°è¿è¡Œ**ï¼šé•¿æ—¶é—´è®­ç»ƒè¯·ä½¿ç”¨ `screen` æˆ– `nohup`

```bash
# ä½¿ç”¨ screen åå°è¿è¡Œ
screen -S llama
GRADIO_SERVER_NAME=0.0.0.0 GRADIO_SERVER_PORT=6006 llamafactory-cli webui
# æŒ‰ Ctrl+A ç„¶åæŒ‰ D åˆ†ç¦»ä¼šè¯

# é‡æ–°è¿æ¥
screen -r llama
```

### ğŸ” éªŒè¯å®‰è£…

```bash
python -c "
import torch
print(f'âœ“ PyTorch: {torch.__version__}')
print(f'âœ“ CUDA å¯ç”¨: {torch.cuda.is_available()}')
print(f'âœ“ CUDA ç‰ˆæœ¬: {torch.version.cuda}')
print(f'âœ“ GPU: {torch.cuda.get_device_name(0)}')

import flash_attn
print(f'âœ“ Flash Attention: {flash_attn.__version__}')

import deepspeed
print(f'âœ“ DeepSpeed: {deepspeed.__version__}')

import bitsandbytes
print(f'âœ“ BitsAndBytes: {bitsandbytes.__version__}')

import peft
print(f'âœ“ PEFT: {peft.__version__}')

import transformers
print(f'âœ“ Transformers: {transformers.__version__}')
"
```

---

## ğŸ“ Changelog | æ›´æ–°æ—¥å¿—

### v1.0.0 (2025-12-19)
- ğŸ‰ Initial release | é¦–æ¬¡å‘å¸ƒ
- âœ… Full RTX 5090 support | å®Œæ•´æ”¯æŒ RTX 5090
- âœ… CUDA 12.8 + PyTorch 2.8 | CUDA 12.8 + PyTorch 2.8
- âœ… Flash Attention 2.8.3 pre-installed | é¢„è£… Flash Attention 2.8.3
- âœ… China mirrors pre-configured | å›½å†…é•œåƒå·²é…ç½®

---

## ğŸ“„ License | è®¸å¯è¯

This project is licensed under the Apache 2.0 License - see the [LICENSE](LICENSE) file for details.

æœ¬é¡¹ç›®é‡‡ç”¨ Apache 2.0 è®¸å¯è¯ - è¯¦è§ [LICENSE](LICENSE) æ–‡ä»¶ã€‚

---

## ğŸ™ Acknowledgments | è‡´è°¢

- [LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory) - Unified Efficient Fine-Tuning of 100+ LLMs
- [AutoDL](https://www.autodl.com/) - GPU Cloud Platform
- [Hugging Face](https://huggingface.co/) - AI Community

---

<div align="center">

**Sponsored by ä¹å¤§å¸ˆé¤é¥®AI**

**Powered by å¾®ä¿¡å…¬ä¼—å·ï¼šå°±æ˜¯AIç§‘æŠ€**

<img src="https://img.shields.io/badge/WeChat-å°±æ˜¯AIç§‘æŠ€-07C160?style=for-the-badge&logo=wechat&logoColor=white" alt="WeChat">

---

â­ If this image helps you, please give it a star! | å¦‚æœè¿™ä¸ªé•œåƒå¯¹ä½ æœ‰å¸®åŠ©ï¼Œè¯·ç»™ä¸ª Starï¼ â­

</div>
